---
layout: post
title: "A Few Notes on Ordinary Least Squares"
date: 2024-11-16
category: [codes]
excerpt: "Although referred to as simple, its strict assumptions and sensitivity to violations often make it anything but simple."
---
We want to model collectivism (GCI) based on ecological threats (i.e. epidemics) and national wealth (GDP). Let's get the data:
  
```r  
ecodata <- read.csv("https://raw.githubusercontent.com/soheilshapouri/epidemics_collectivism/main/Data%20S2.csv")
```
If we have more predictors than observations, we will switch to regilarized regression. But that is not the case here so we move on to train-test split. 
```r
set.seed(123)  
index <- sample(nrow(ecodata), round(nrow(ecodata)*0.8))  
eco_train <- ecodata[index, ]  
eco_test <- ecodata[-index, ]  
```
Start with a simple linear regression
```r
model1 <- lm(GCI ~ No_Epidemics, data = eco_train)
```
Even for simple linear regression there is a better way to do it, using cross-validation to get more reliable assessment of model performance.  
```r
library(caret)
set.seed(123)
cv_model1 <- train(
  form = GCI ~ No_Epidemics, 
  data = eco_train,
  method = "lm", 
  trControl = trainControl(method = "cv", number = 10)
)
```
Now that we have the model, We can extract its information.
```r
# regression coefficients
coef(cv_model1$finalModel)
#RMSE
sigma(cv_model1$finalModel)
#confidence interval
confint(cv_model1$finalModel)

# and the mode important thing, getting the model summary 
summary(cv_model1$finalModel)
# where Residual standard error is RMSE 
# and R-squared is the amount of variance in response explained by explanatory variable(s)
```
If the goal is inference, very low R-squared tells me this is not a good model. If the goal is prediction, considering the range of reponse "GCI", which is -1.85 to +1.92 RMSE of 0.72 is pretty high.
For both cases, I would add more predcitors to see whether I can explain more variability or lower RMSE.  
```r
cv_model2 <- train(
  form = GCI ~ No_Epidemics + GDP,
  data = eco_train,
  method = "lm",
  trControl = trainControl(method = "cv", number = 10)
)
summary(cv_model2$finalModel)
```
R-squared was increased while RMSE was decreased. We are in the right direction but we should always consider the possibility of interactions.  
```r
cv_model3 <- train(
  form = GCI ~ No_Epidemics + GDP + No_Epidemics:GDP, 
  data = eco_train,
  method = "lm", 
  trControl = trainControl(method = "cv", number = 10)
)
summary(cv_model3$finalModel)
```
A small increase in R2 and a decrease in RMSE is noticed. But before moving further let's create a visualization of model 2. A contour plot for two main effects and predicted GCI. 
```r
grid <- expand.grid(
  No_Epidemics = seq(min(eco_train$No_Epidemics), max(eco_train$No_Epidemics), length.out = 100),
  GDP = seq(min(eco_train$GDP), max(eco_train$GDP), length.out = 100)
)

grid$Predicted_GCI <- predict(cv_model2$finalModel, newdata = grid)

library(ggplot2)

ggplot(grid, aes(x = No_Epidemics, y = GDP, z = Predicted_GCI)) +
  geom_tile(aes(fill = Predicted_GCI)) +
  scale_fill_gradient(low = "#F6BE00", high = "#302D26", name = "Predicted GCI") + # Yellowish gradient
  geom_contour(color = "white", alpha = 0.7) + # Add white contour lines
  labs(
    title = "Main Effects of No_Epidemics and GDP on Predicted GCI",
    x = "No_Epidemics",
    y = "GDP"
  ) +
  theme_minimal(base_size = 14) + # Use a minimal theme
  theme(
    plot.title = element_text(face = "bold", size = 18, hjust = 0.5, color = "#F6BE00"),
    axis.title = element_text(color = "#F6BE00"),
    legend.title = element_text(color = "#F6BE00"),
    legend.text = element_text(color = "#F6BE00")
  )
```  
![Contour Plot](https://raw.githubusercontent.com/soheilshapouri/soheilshapouri.github.io/master/_posts/contour.jpeg)  
Now we have three models and I want to compare them systematically, looking at their average performance across test sets of cross-validation. 
```r
summary(resamples(list(
  cv_model1,
  cv_model2,
  cv_model3
)))
```
Call:
summary.resamples(object = resamples(list(cv_model1, cv_model2, cv_model3)))

Models: Model1, Model2, Model3 
Number of resamples: 10 

MAE 
            Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's
Model1 0.4833124 0.5331560 0.6010222 0.5973014 0.6383185 0.7729095    0
Model2 0.3564544 0.4050053 0.4700918 0.4610663 0.4841861 0.5811967    0
Model3 0.3113311 0.4364668 0.4670282 0.4658248 0.5073480 0.5909254    0

RMSE 
            Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's
Model1 0.5794827 0.6453125 0.7162247 0.7341401 0.7505635 1.1036097    0
Model2 0.4260384 0.4812766 0.5759897 0.5725600 0.6206778 0.7570160    0
Model3 0.3609176 0.5378885 0.5875829 0.5824346 0.6134648 0.8490884    0

Rsquared 
              Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's
Model1 0.001823167 0.1466940 0.2510966 0.2395185 0.3390086 0.4543522    0
Model2 0.269115277 0.4296735 0.4719394 0.4749588 0.5498556 0.6854996    0
Model3 0.107911144 0.4198229 0.5131655 0.5034710 0.6477952 0.7573449    0  
Obvuisly model 2 is the best in terms of R2 and RMSE. 




















